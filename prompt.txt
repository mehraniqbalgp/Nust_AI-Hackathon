ANONYMOUS CAMPUS RUMOR VERIFICATION SYSTEM: COMPLETE DESIGN DOCUMENTEXECUTIVE SUMMARYThis document presents a comprehensive design for a decentralized, anonymous campus rumor verification system that addresses the fundamental challenge: How do you establish truth in an anonymous system where anyone can participate, no one is in charge, and coordinated groups might attempt manipulation?The solution combines cryptographic identity management, game-theoretic incentive design, temporal decay mechanisms, and statistical anomaly detection to create a self-regulating ecosystem where truth emerges from competitive verification rather than popularity contests.1. PROBLEM DECOMPOSITION & CORE CHALLENGES1.1 The Fundamental Paradoxes We Must SolveParadox #1: Anonymous Accountability

CHALLENGE: Prevent duplicate voting without knowing who anyone is
WHY IT'S HARD: Traditional identity verification requires collecting personal data, which breaks anonymity
Paradox #2: Truth vs. Popularity

CHALLENGE: Popular false information shouldn't win just because many people believe it
WHY IT'S HARD: Democratic voting naturally favors popular opinions, not factual accuracy
Paradox #3: Sybil Resistance Without Gatekeepers

CHALLENGE: Prevent one person from creating 100 fake accounts to manipulate votes
WHY IT'S HARD: Without a central authority checking IDs, anyone can create unlimited accounts
Paradox #4: Temporal Consistency

CHALLENGE: Verified facts from last month are changing scores (this should NOT happen)
WHY IT'S HARD: New evidence might emerge, but coordinated attacks can also manipulate historical records
Paradox #5: Ghost Data Pollution

CHALLENGE: Deleted rumors still affecting trust scores of new rumors
WHY IT'S HARD: Graph-based systems create dependencies that persist even after node deletion
Paradox #6: Proving Un-gameability

CHALLENGE: Mathematically prove coordinated liars can't win
WHY IT'S HARD: Game theory shows coordinated groups usually have advantages over individuals
2. FOUNDATIONAL ASSUMPTIONS & DESIGN PRINCIPLES2.1 What We Assume About Our EnvironmentASSUMPTION 1: Campus Network Boundary

Students access the system through campus WiFi/network infrastructure
The campus network has some natural rate-limiting (can't create infinite connections instantly)
This provides a weak Sybil resistance layer without collecting identity
ASSUMPTION 2: Honest Minority Exists

At least 30% of active participants are truth-seeking (not coordinated attackers)
This is realistic: most students want accurate information about campus events
ASSUMPTION 3: Cost Asymmetry

Creating/verifying true information has different cost structure than creating false information
Truth-tellers can provide evidence; liars must coordinate stories
ASSUMPTION 4: Temporal Locality of Information

Campus rumors have natural time relevance (event-based)
Recent information matters more than old information
Information value decays predictably
ASSUMPTION 5: Statistical Detectability of Coordination

Groups coordinating to manipulate scores exhibit detectable behavioral patterns
Natural user behavior differs from scripted bot behavior
2.2 Design Principles That Guide Every DecisionPRINCIPLE 1: Trust Through Skin-in-the-Game

Users must risk something valuable (reputation capital) when making claims
Correct claims earn rewards; incorrect claims impose penalties
PRINCIPLE 2: Decentralized Verification Through Competition

Multiple independent verification pathways compete
No single point of failure or control
PRINCIPLE 3: Temporal Decay & Fresh Evidence Priority

All trust scores naturally decay over time
System continuously re-evaluates based on new evidence
PRINCIPLE 4: Transparency of Mechanism, Opacity of Identity

All algorithms and rules are public and auditable
Individual user identities remain anonymous
PRINCIPLE 5: Economic Game Theory Over Social Trust

Design incentives where rational self-interest aligns with truth-telling
Don't rely on altruism or social pressure
3. SYSTEM ARCHITECTURE: THE COMPLETE TECHNICAL DESIGN3.1 Three-Layer Architecture Overviewâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PRESENTATION LAYER (User Interface)           â”‚
â”‚  - Anonymous submission forms                           â”‚
â”‚  - Trust score visualization                            â”‚
â”‚  - Evidence attachment interfaces                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        APPLICATION LAYER (Business Logic)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚Trust Score   â”‚ â”‚Game Theory   â”‚ â”‚Anti-Sybil   â”‚    â”‚
â”‚  â”‚Engine        â”‚ â”‚Incentive Mgr â”‚ â”‚Detection    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚Evidence      â”‚ â”‚Temporal Decayâ”‚ â”‚Anomaly      â”‚    â”‚
â”‚  â”‚Verification  â”‚ â”‚Manager       â”‚ â”‚Detection    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          DATA LAYER (Distributed Storage)               â”‚
â”‚  - Blockchain/Merkle-DAG for rumor immutability         â”‚
â”‚  - Distributed hash table for evidence storage          â”‚
â”‚  - Local reputation ledgers (zero-knowledge proofs)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜3.2 Identity Management: Anonymous But UniqueTHE CORE MECHANISM: Zero-Knowledge Credential SystemWhen a student first joins:STEP 1: Credential Issuance (One-Time, Anonymous)
1. Student proves they're a campus member (scans student ID to local device)
2. Device generates cryptographic keypair (private key never leaves device)
3. Campus authentication server signs a "campus membership credential"
   - Server sees: "A valid student requested a credential"
   - Server does NOT see: Which specific student or their public key
4. Device stores credential locally (like a digital passport)STEP 2: Anonymous Participation
Every time user submits/votes:
1. Device generates zero-knowledge proof: "I possess a valid campus credential"
2. Proof is verifiable by anyone but reveals nothing about identity
3. Proof includes a unique nullifier tag derived from:
   - Private key (unique to user)
   - Specific action context (e.g., "voting on rumor #12345")
4. System checks: "Has this nullifier been seen before for this action?"
   - If YES: Reject (duplicate vote)
   - If NO: Accept and record nullifierWHY THIS WORKS:

Each student has exactly ONE credential (tied to student status verification)
Each action generates a unique nullifier (can't vote twice on same rumor)
Nullifiers reveal nothing about identity (cryptographic one-way function)
System prevents duplicate voting without knowing who anyone is
TECHNICAL IMPLEMENTATION:

Use zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge)
Merkle tree of all issued credentials (proves credential is in valid set)
Nullifier = Hash(PrivateKey + ActionID) ensures uniqueness per action
3.3 The Trust Score Algorithm: Multi-Dimensional ReputationTRUST SCORE IS NOT A SINGLE NUMBERâ€”IT'S A VECTOR WITH 5 COMPONENTS:TrustScore(rumor) = {
  Veracity Score (V),    // How likely true based on evidence
  Confidence Score (C),   // How certain we are about veracity
  Temporal Relevance (T), // How current/relevant the information is
  Source Reliability (S), // Track record of submitter
  Network Consensus (N)   // Agreement across independent verifiers
}3.3.1 Component 1: Veracity Score (V)THE CORE IDEA: Evidence-based scoring where different types of evidence carry different weights.Evidence Types & Base Weights:
1. FIRST-HAND TESTIMONY (Weight: 0.3 per witness)
   - "I personally attended the event and saw X"
   - Must include temporal proof (timestamp from device)
   - Diminishing returns: 1st witness = 0.3, 2nd = 0.2, 3rd = 0.15

2. PHOTOGRAPHIC/VIDEO EVIDENCE (Weight: 0.5 per item)
   - Metadata must be verifiable (time, location if available)
   - Cryptographic hash stored immutably
   - Reverse image search check for prior existence

3. DOCUMENTARY EVIDENCE (Weight: 0.6 per document)
   - Official emails, announcements, documents
   - Must be verifiable against known campus communication channels

4. STATISTICAL CORROBORATION (Weight: 0.4)
   - When multiple independent rumors align on same facts
   - Cross-referencing mechanism (explained in 3.3.5)

5. EXPERT VERIFICATION (Weight: 0.7)
   - Users with high historical accuracy scores
   - Domain-specific expertise (determined by past performance)Formula:
V = Î£(evidence_weight Ã— evidence_quality Ã— recency_factor) / (1 + contradiction_penalty)

Where:
- evidence_quality = automated checks (image forensics, metadata validation)
- recency_factor = e^(-Î»t) where t = hours since evidence submitted, Î» = 0.01
- contradiction_penalty = Î£(contradicting_evidence_weights)EXAMPLE CALCULATION:Rumor: "The library will be closed tomorrow for renovations"

Supporting Evidence:
- 2 first-hand testimonies from students who saw signs: 0.3 + 0.2 = 0.5
- 1 photo of official closure notice: 0.5
- 1 student who works at library confirms: 0.7 (expert verification)

V = (0.5 Ã— 1.0 Ã— 0.98) + (0.5 Ã— 0.95 Ã— 1.0) + (0.7 Ã— 1.0 Ã— 1.0)
V = 0.49 + 0.475 + 0.7 = 1.665

Contradicting Evidence:
- 1 testimony claiming library open: 0.3

Final V = 1.665 / (1 + 0.3) = 1.28
Normalized to 0-1 scale: min(V/2, 1.0) = 0.64 â†’ 64% veracity3.3.2 Component 2: Confidence Score (C)THE CORE IDEA: How certain should we be about our veracity assessment?Factors Affecting Confidence:C = base_confidence Ã— sample_size_factor Ã— time_factor Ã— diversity_factor

1. BASE CONFIDENCE (from evidence type)
   - Documentary: 0.9 (hard to fake official documents)
   - Photo/Video: 0.7 (can be manipulated but requires effort)
   - Testimony: 0.5 (easiest to fabricate)

2. SAMPLE SIZE FACTOR
   - More independent verifications = higher confidence
   - Formula: 1 - e^(-n/10) where n = number of independent verifiers
   - Asymptotically approaches 1.0 as n increases

3. TIME FACTOR
   - Recent verifications more reliable than old ones
   - Formula: 0.5 + 0.5e^(-t/24) where t = hours since last verification

4. DIVERSITY FACTOR
   - Are verifiers independent or clustered?
   - Measure temporal clustering: did all votes come within 5 minutes?
   - Measure network clustering: are verifiers typically correlated?
   - Formula: 1 - (clustering_coefficient)EXAMPLE:
Rumor with:
- 5 independent verifiers (sample_size_factor = 1 - e^(-0.5) = 0.39)
- Verified 2 hours ago (time_factor = 0.5 + 0.5e^(-2/24) = 0.96)
- Low clustering (diversity_factor = 0.8)

C = 0.7 Ã— 0.39 Ã— 0.96 Ã— 0.8 = 0.21 â†’ 21% confidence

This means: "We think this is probably true (64% veracity) but we're not very confident (21%) because we don't have enough diverse verification yet."3.3.3 Component 3: Temporal Relevance (T)THE CORE IDEA: Information value decays over time, but the decay rate depends on content type.Decay Curves for Different Rumor Types:EVENT-BASED RUMORS (parties, meetings, deadlines):
T = e^(-0.5t) where t = hours since event time
// Rapid decay after event passes

POLICY/RULE CHANGES (new requirements, schedule changes):
T = 1 / (1 + 0.01t) where t = days since announcement
// Slower decay, remains relevant for semester

PERSON-BASED RUMORS (gossip, personal information):
T = 0.5 + 0.5e^(-0.1t) where t = days since submission
// Never goes to zero (always somewhat relevant)WHY THIS MATTERS FOR "MYSTERIOUSLY CHANGING SCORES":SCENARIO: "Professor X cancelled Friday's class"
- Posted Monday, high relevance (T = 0.95)
- Friday passes, relevance drops (T = 0.20)
- Trust score APPEARS to change, but it's INTENTIONAL decay
- This prevents old information from polluting current feed

SOLUTION TO "MYSTERIOUS CHANGES":
- Display TWO scores:
  1. INTRINSIC TRUST: "How trustworthy was this when relevant?"
  2. CURRENT RELEVANCE: "How relevant is this now?"
- Archive old rumors with locked intrinsic trust score
- Only current relevance decays3.3.4 Component 4: Source Reliability (S)THE CORE IDEA: Track anonymous user reputation without revealing identity.Reputation Accumulation Mechanism:Each user's device maintains LOCAL reputation ledger:
{
  total_submissions: 47,
  verified_accurate: 38,
  verified_false: 5,
  still_disputed: 4,
  reputation_score: 0.76
}

Reputation formula:
S = (verified_accurate - 2Ã—verified_false) / (total_submissions) Ã— recency_weight

Where:
- Penalize false claims TWICE as much as we reward accurate claims
- Recent behavior weighted more: recency_weight = Î£(w_i Ã— age_factor_i)Reputation Proof Mechanism:
When submitting new rumor:
1. Device generates zero-knowledge proof: "My reputation score is S"
2. Proof is verifiable but doesn't reveal submission history
3. High-reputation users' submissions start with higher initial trustCRITICAL ANTI-GAMING FEATURE:
Reputation decay over inactivity:
- If user doesn't participate for 30 days, reputation decays 10%/week
- Prevents "reputation hoarding" then mass manipulation
- Encourages continuous honest participation3.3.5 Component 5: Network Consensus (N)THE CORE IDEA: Independent verification pathways should agree.Measuring Consensus Across Different Verifier Groups:Partition verifiers into independent groups:
- GROUP A: Verifiers who joined before rumor posted (pre-existing users)
- GROUP B: Verifiers with high historical accuracy (experts)
- GROUP C: Verifiers with low correlation to each other (diverse perspectives)

Consensus Score:
N = Agreement(A, B, C) Ã— (1 - suspicious_pattern_penalty)

Agreement formula:
If all groups vote same direction (all support OR all dispute):
  N = min(|votes_A|, |votes_B|, |votes_C|) / max(|votes_A|, |votes_B|, |votes_C|)

If groups disagree:
  N = 0.3 (low consensus, disputed claim)DETECTING SUSPICIOUS PATTERNS:RED FLAGS that reduce consensus score:

1. TEMPORAL CLUSTERING
   - All votes within 2-minute window: -0.3 penalty
   - Suggests coordinated group action

2. BEHAVIORAL SIMILARITY
   - All voters have similar voting history patterns: -0.4 penalty
   - Uses cosine similarity of vote vectors

3. NETWORK POSITION CLUSTERING
   - All voters typically vote on same subset of rumors: -0.5 penalty
   - Graph analysis reveals coordinated groups

4. UNNATURAL VOTING PATTERN
   - All votes exactly 5 minutes apart: -0.6 penalty
   - Suggests automated bot behavior3.4 The Complete Trust Score IntegrationFINAL TRUST SCORE FORMULA:TrustScore = weighted_sum + boost_penalties

weighted_sum = (
  0.35 Ã— V +    // Veracity is most important
  0.25 Ã— C +    // But confidence matters
  0.20 Ã— T +    // Temporal relevance crucial for UX
  0.10 Ã— S +    // Source reputation provides prior
  0.10 Ã— N      // Network consensus breaks ties
) Ã— 100

boost_penalties:
+ official_verification_boost (if campus admin verified: +15)
+ expert_consensus_boost (if 5+ high-rep users agree: +10)
- bot_detection_penalty (if automated behavior detected: -20)
- contradiction_penalty (if strong contradicting evidence: -25)EXAMPLE COMPLETE CALCULATION:Rumor: "Campus wifi will be down Saturday 2-4pm for maintenance"

Component Scores:
V = 0.82 (strong evidence: official email screenshot + 3 testimonies)
C = 0.65 (moderate confidence: only 3 verifiers so far)
T = 0.95 (highly relevant: Saturday is tomorrow)
S = 0.71 (submitter has good track record)
N = 0.58 (moderate consensus: diverse groups agree but sample size small)

weighted_sum = (0.35Ã—0.82 + 0.25Ã—0.65 + 0.20Ã—0.95 + 0.10Ã—0.71 + 0.10Ã—0.58) Ã— 100
             = (0.287 + 0.163 + 0.19 + 0.071 + 0.058) Ã— 100
             = 0.769 Ã— 100
             = 76.9

Boosts/Penalties:
+ Official email screenshot matches known IT department format: +10
+ No bot behavior detected: 0
- No contradictions: 0

FINAL TRUST SCORE = 76.9 + 10 = 86.9 / 100HOW USERS SEE THIS:â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”” Campus WiFi Maintenance Tomorrow                  â”‚
â”‚                                                       â”‚
â”‚ Trust Score: 87/100 â­â­â­â­ (HIGH CONFIDENCE)        â”‚
â”‚                                                       â”‚
â”‚ Evidence: ğŸ“§ Official Email + ğŸ‘¥ 3 Witnesses         â”‚
â”‚ Relevance: â° Happening in 18 hours                  â”‚
â”‚ Consensus: âœ… 12 verify / âŒ 1 dispute                â”‚
â”‚                                                       â”‚
â”‚ [View Evidence] [Add Verification] [Dispute]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜4. GAME-THEORETIC INCENTIVE DESIGN4.1 The Fundamental Game Theory ProblemTRADITIONAL VOTING PARADOX:
In normal systems:
- Voting costs nothing
- Users vote based on belief/preference
- Coordinated groups can overwhelm truth

Our system must create different incentives where:
- Truth-telling is profitable
- Lying is expensive
- Coordinated lying is detectably expensive4.2 Staking Mechanism: Skin in the GameREPUTATION AS CRYPTOCURRENCY:Every user starts with 100 "TruthTokens" (reputation capital)WHEN SUBMITTING A RUMOR:
User must STAKE tokens on their claim:
- Minimum stake: 5 tokens
- User chooses stake amount (5-50 tokens)
- Higher stake = higher initial visibility

If rumor is eventually verified TRUE:
  Return stake + bonus (stake Ã— 0.5)
  Example: Stake 20, get back 30

If rumor is eventually verified FALSE:
  Lose entire stake
  Example: Stake 20, lose all 20

If rumor remains disputed (no consensus after 48 hours):
  Return 50% of stake
  Example: Stake 20, get back 10WHEN VERIFYING/DISPUTING A RUMOR:
Verifiers also stake tokens:
- Minimum stake: 2 tokens
- Must provide evidence (can't just vote blindly)

If your verification matches eventual outcome:
  Return stake + bonus from liars' lost stakes
  Example: Stake 5, get back 7

If your verification contradicts eventual outcome:
  Lose stake
  Example: Stake 5, lose all 5

Outcome determined by:
- Veracity score > 0.7 = "verified true"
- Veracity score < 0.3 = "verified false"
- 0.3 â‰¤ score â‰¤ 0.7 = "disputed" (partial refunds)EARNING MORE TOKENS:
Users earn tokens through:
1. Accurate submissions that get verified (+10-50 tokens)
2. Early accurate verification of others' claims (+2-10 tokens)
3. Providing high-quality evidence (+5-15 tokens)
4. Consistent accuracy over time (monthly bonus +20 tokens)

Token cap: Maximum 500 tokens per user
- Prevents reputation hoarding
- Encourages continuous participation4.3 Why This Defeats Coordinated Liars: Game Theory ProofATTACK SCENARIO: 20 coordinated liars try to push false rumorSetup:
- 20 attackers each have 100 tokens (2000 total)
- 100 honest users, each with 100 tokens (10,000 total)
- Attackers want to make "Campus closing permanently" go viral

Attack Strategy 1: Overwhelming Initial Vote
Step 1: Submit false rumor, stake 50 tokens
Step 2: All 20 attackers verify, each staking 10 tokens (200 tokens total)
Step 3: Initial trust score appears high (20 verifications!)

DEFENSIVE MECHANISMS ACTIVATE:

A) TEMPORAL CLUSTERING DETECTION
   - All 20 votes within 5-minute window
   - Suspicious pattern penalty: -0.6 to consensus score
   - Trust score: heavily penalized despite vote count

B) HONEST USER COUNTER-VERIFICATION
   - First honest user disputes with evidence (photo showing campus open)
   - Evidence weight (0.5) vs. Testimony weight (0.3)
   - Veracity score starts dropping

C) REPUTATION DECAY MECHANISM
   - Attackers must maintain false narrative for 48 hours
   - Temporal relevance decays: T(t) = e^(-0.5t)
   - Must keep staking tokens to maintain visibility

D) ECONOMIC EXHAUSTION
   - Honest users continue providing contradicting evidence
   - Veracity score drops below 0.3 ("verified false")
   - All attackers lose their staked tokens:
     * Original submitter: -50 tokens
     * 20 verifiers: -10 each = -200 tokens
     * Total loss: 250 tokens per attack attempt

E) REPUTATION DAMAGE
   - Each attacker's source reliability (S) score drops
   - Future submissions start with lower trust
   - Eventually attackers run out of tokensMATHEMATICAL PROOF OF UN-GAMEABILITY:Theorem: A coordinated group of L liars cannot sustainably maintain false information in equilibrium against H honest users if H > 2L and the evidence quality ratio exceeds the numbers ratio.Proof:Let:
- L = number of coordinated liars
- H = number of honest users
- E_h = average evidence quality of honest users
- E_l = average evidence quality of liars (testimonies only, no real evidence)
- T = token pool of liars
- T_h = token pool of honest users

For false rumor to maintain Veracity Score > 0.5:

Î£(liar_evidence Ã— weights) > Î£(honest_evidence Ã— weights)

But evidence weights favor quality:
- Real evidence (photos, documents): weight 0.5-0.7
- Testimony alone: weight 0.3

Liars cannot produce real evidence (doesn't exist), so:
L Ã— 0.3 Ã— stake_l > H Ã— 0.6 Ã— stake_h

For liars to win economically:
L Ã— stake_l < liar_token_pool (T)

Combining:
L Ã— stake_l > 2 Ã— H Ã— stake_h (to overcome evidence quality gap)
L Ã— stake_l < T (to be sustainable)

Therefore: T > 2 Ã— H Ã— stake_h

But honest users can collectively stake token pool T_h
If T_h > T, honest users can always outbid liars

Given our assumption H > 2L:
T_h = H Ã— average_tokens > 2L Ã— average_tokens = 2T

Therefore: T_h > 2T > required threshold

CONCLUSION: Liars cannot sustainably win if honest users participate.

Q.E.D.IN SIMPLE TERMS:
Liars lose because:
1. They can only provide weak evidence (testimonies)
2. Honest users provide strong evidence (photos, documents, reality)
3. Coordinated behavior is detectable and penalized
4. Each failed attempt costs tokens
5. Honest users outnumber liars and collectively have more tokens
6. Eventually liars run out of ammunition (tokens)5. PREVENTING SYBIL ATTACKS (BOT ACCOUNTS)5.1 Multi-Layer Sybil DefenseLAYER 1: Credential Scarcity (Primary Defense)
Remember: Each user needs a campus-verified credential
- Only real students can get credentials
- One credential per student ID
- Campus identity verification process prevents mass account creation

Attack cost: Requires stealing/faking student IDs (very expensive)LAYER 2: Behavioral Analysis (Bot Detection)BEHAVIORAL FINGERPRINTING:
Each user's activity pattern creates a "behavioral signature":

1. TEMPORAL PATTERNS
   - Human: Irregular timing, aligned with sleep/class schedules
   - Bot: Perfectly regular intervals (every 5 minutes)
   
   Measure: Shannon entropy of inter-action time deltas
   Human score: 0.7-0.9 (high entropy, random)
   Bot score: 0.1-0.3 (low entropy, predictable)

2. ACTION SEQUENCES
   - Human: Read rumor â†’ Think â†’ Vote â†’ Maybe add evidence
   - Bot: Vote immediately, same pattern every time
   
   Measure: Markov chain transition probabilities
   Human: High variability in action sequences
   Bot: Repetitive state machine behavior

3. CONTENT INTERACTION
   - Human: Reading time correlates with content length
   - Bot: Fixed time regardless of content
   
   Measure: Time-on-page vs. content-length correlation
   Human: Strong positive correlation (r > 0.7)
   Bot: No correlation (r < 0.2)

4. NETWORK POSITION
   - Human: Votes on diverse topics, different times
   - Bot network: Highly clustered, votes on same rumors
   
   Measure: Graph centrality and clustering coefficient
   Humans: Scattered across graph
   Bots: Form dense subgraphsBOT SCORE CALCULATION:
BotScore = weighted_average(
  0.3 Ã— temporal_regularity_score,
  0.3 Ã— behavioral_repetition_score,
  0.2 Ã— interaction_anomaly_score,
  0.2 Ã— network_clustering_score
)

If BotScore > 0.7:
  - Flag account for review
  - Reduce vote weight by 90%
  - Prevent new submissions
  - Quarantine existing votesLAYER 3: Proof-of-Work for Actions (Rate Limiting)Each action requires computational puzzle:
- Easy enough for phones to solve (0.5-2 seconds)
- Hard enough to prevent mass automation

When user submits/votes:
1. Client receives challenge: "Find nonce where Hash(nonce + action) starts with 000"
2. Client performs computation (takes ~1 second on phone)
3. Submit action + proof-of-work solution

This prevents:
- Rapid-fire bot voting (each vote costs computation)
- Mass simultaneous account actions (distributed cost)

Attack cost: Requires significant computational resources at scaleLAYER 4: Economic Cost (Token Staking)Remember: Every action requires staking tokens
- Submitting rumor: 5-50 tokens
- Verifying/disputing: 2-10 tokens

Creating 100 bot accounts:
- Each starts with 100 tokens (generous initial allocation)
- Total: 10,000 tokens across 100 accounts

Single coordinated attack:
- 100 bots each vote: 100 Ã— 5 = 500 tokens staked
- If attack fails (likely due to honest users): LOSE all 500 tokens
- Bots can run ~20 attacks before depleting resources

Attack sustainability: Very limited, economic suicide strategyLAYER 5: Progressive Trust (New Account Limitations)New accounts have restricted capabilities:

Week 1:
- Can vote on rumors (stake limit: 5 tokens)
- Cannot submit new rumors
- Vote weight: 50% of normal

Week 2-4:
- Can submit rumors (stake limit: 10 tokens)
- Vote weight: 75% of normal

After 30 days of consistent activity:
- Full capabilities unlocked
- Vote weight: 100%
- Stake limits removed

Bot impact: Even if created, bots have very limited influence initially6. SOLVING THE "GHOST DATA" PROBLEM6.1 Understanding the BugTHE PROBLEM:
Current state:
- Rumor A exists, gets trust score 0.8
- Rumor B references Rumor A in evidence ("As previously reported...")
- Rumor C also references Rumor A
- User deletes Rumor A
- Rumor B and C STILL showing inflated trust scores from deleted reference

Why this is bad:
- Creates "phantom evidence" that users can't verify
- Enables attack: Post fake rumor â†’ Get it verified â†’ Use as "evidence" â†’ Delete original6.2 Immutable Core + Visibility Layers SolutionARCHITECTURE CHANGE:Instead of deleting rumors, we use TWO-LAYER system:

LAYER 1: IMMUTABLE LEDGER (Blockchain/Merkle DAG)
- Every rumor permanently recorded with cryptographic hash
- Cannot be deleted or modified
- Public and verifiable

LAYER 2: VISIBILITY/REPUTATION LAYER
- Controls what users SEE
- Rumors can be "deprecated" (hidden from main feed)
- But original data remains accessible via direct link

When user "deletes" rumor:
1. Rumor marked as [DEPRECATED] in visibility layer
2. Removed from main feed and search
3. But cryptographic hash remains in immutable ledger
4. Other rumors referencing it show: "References deprecated rumor [view]"TRUST SCORE RECALCULATION ON DEPRECATION:When Rumor A is deprecated:

For each rumor that referenced A:
1. Identify all trust score contributions from A
2. Reduce those contributions by deprecation_penalty = 0.7
3. Recalculate dependent trust scores

Example:
Rumor B originally had:
- Own evidence: 0.6 veracity
- Reference to Rumor A: +0.2 veracity boost
- Total: 0.8

After A deprecated:
- Own evidence: 0.6 (unchanged)
- Reference to deprecated A: +0.2 Ã— 0.7 = +0.14 (reduced)
- New total: 0.74

Users see:
"âš ï¸ This rumor references deprecated content. Trust score adjusted."GRAPH DEPENDENCY TRACKING:Maintain directed acyclic graph (DAG) of rumor dependencies:

Rumor A â”€â”€â”¬â”€â”€> Rumor B
          â”‚
          â”œâ”€â”€> Rumor C
          â”‚
          â””â”€â”€> Rumor D

When A is deprecated:
1. Traverse all outgoing edges (B, C, D)
2. Propagate deprecation penalty downstream
3. Prevent cycles (DAG structure enforces this)
4. Update all dependent scores atomically

Data structure:
{
  rumor_id: "A",
  status: "deprecated",
  references: ["B", "C", "D"],
  deprecation_timestamp: 1234567890,
  original_hash: "0x4f3a2..."  // Immutable proof of original content
}7. FIXING "MYSTERIOUSLY CHANGING SCORES"7.1 Root Cause AnalysisWHY SCORES CHANGE:Legitimate reasons:
1. New evidence added (EXPECTED)
2. Temporal decay (EXPECTED)
3. Source reputation updated based on new data (EXPECTED)
4. More verifiers participated (EXPECTED)

Illegitimate reasons:
5. Coordinated attack retroactively manipulating old rumors (BUG)
6. Improper graph dependency causing cascade effects (BUG)7.2 Solution: Immutable Checkpoints + Audit TrailCHECKPOINT MECHANISM:Every 24 hours, create immutable checkpoint:

Checkpoint = {
  timestamp: 1234567890,
  rumor_snapshots: {
    "rumor_123": {
      trust_score: 0.87,
      component_scores: {V: 0.82, C: 0.65, T: 0.95, S: 0.71, N: 0.58},
      evidence_hash: "0x8a4f...",
      verifier_count: 12,
      cryptographic_signature: "0x7b2e..."
    },
    "rumor_456": { ... }
  },
  merkle_root: "0x3d9a..."  // Proves integrity of entire checkpoint
}

Stored in: Distributed immutable ledger (blockchain or similar)WHAT THIS ENABLES:1. HISTORICAL QUERIES
   User can ask: "What was the trust score on 2/1/2026?"
   System retrieves checkpoint from that date
   
2. ANOMALY DETECTION
   Compare current score to checkpoint 24 hours ago:
   If Î”score > 0.3 without new evidence: FLAG SUSPICIOUS
   
3. ROLLBACK PROTECTION
   Cannot retroactively change historical scores
   Attackers can only affect current/future scores

4. TRANSPARENCY
   User sees: 
   "Current score: 0.72 (was 0.85 yesterday)"
   "Change due to: [Temporal decay: -0.10] [New contradicting evidence: -0.03]"AUDIT TRAIL VISUALIZATION:â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Trust Score History: "Library Closure"                  â”‚
â”‚                                                          â”‚
â”‚  1.0 â”¤                                                   â”‚
â”‚      â”‚     â—â”€â”€â”€â”€â—                                        â”‚
â”‚  0.8 â”¤    â•±      â•²                                       â”‚
â”‚      â”‚   â•±        â•²                                      â”‚
â”‚  0.6 â”¤  â—          â—â”€â”€â”€â”€â”€â—                               â”‚
â”‚      â”‚                    â•²                              â”‚
â”‚  0.4 â”¤                     â—                             â”‚
â”‚      â”‚                                                   â”‚
â”‚  0.2 â”¤                                                   â”‚
â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€> Time                â”‚
â”‚         2/1  2/2  2/3  2/4  2/5                          â”‚
â”‚                                                          â”‚
â”‚ Score changes explained:                                â”‚
â”‚ 2/1â†’2/2: +0.05 (new photo evidence added)               â”‚
â”‚ 2/2â†’2/3: -0.02 (temporal decay, event approaching)      â”‚
â”‚ 2/3â†’2/4: -0.15 (event passed, relevance dropped)        â”‚
â”‚ 2/4â†’2/5: -0.10 (continued temporal decay)               â”‚
â”‚                                                          â”‚
â”‚ [View Full Audit Trail] [Export Data]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜7.3 Write-Once, Read-Many Evidence StoragePREVENTING EVIDENCE TAMPERING:When evidence is submitted:

1. CRYPTOGRAPHIC HASHING
   evidence_hash = SHA256(content + timestamp + submitter_nullifier)
   
2. IMMUTABLE STORAGE
   Store in distributed hash table (DHT) or IPFS
   Content-addressed: fetch by hash, cannot modify without changing hash
   
3. MERKLE TREE INCLUSION
   Add hash to Merkle tree of all evidence
   Merkle root published in checkpoint
   
4. PROOF OF EXISTENCE
   Anyone can verify: "This evidence existed at time T"
   Cannot claim evidence was added retroactively

Result: Evidence history is auditable and tamper-proof8. ANTI-MANIPULATION: ADVANCED DEFENSES8.1 Statistical Anomaly DetectionCONTINUOUS MONITORING SYSTEM:Monitor these statistical metrics in real-time:

1. VOTE VELOCITY ANOMALIES
   Normal: Votes trickle in over hours/days
   Attack: Sudden spike in votes
   
   Detection:
   - Calculate baseline: average votes per hour for this rumor type
   - Current rate: votes in last 10 minutes
   - If current_rate > 5 Ã— baseline: ANOMALY FLAG

2. VOTE CORRELATION ATTACKS
   Normal: Users vote independently
   Attack: Coordinated group always votes together
   
   Detection:
   - Build co-voting matrix: How often do users vote together?
   - Calculate correlation coefficient for each user pair
   - If cluster of users has avg correlation > 0.8: SYBIL FLAG

3. EVIDENCE QUALITY DIVERGENCE
   Normal: Evidence quality matches claims
   Attack: Many votes but weak/no evidence
   
   Detection:
   - Ratio = (sum of vote counts) / (sum of evidence weights)
   - Normal ratio: 2-5 votes per unit of evidence
   - If ratio > 10: SUSPICIOUS (lots of votes, little evidence)

4. TEMPORAL PATTERN REGULARITY
   Normal: Human activity follows circadian rhythms
   Attack: Bots operate 24/7 with regular intervals
   
   Detection:
   - Fourier analysis of user activity timestamps
   - Human patterns: Peaks at lunch/evening, quiet at night
   - Bot patterns: Uniform distribution or perfect periodicityAUTOMATED RESPONSE SYSTEM:When anomaly detected:

SEVERITY 1 (Minor anomaly):
â†’ Increase scrutiny (require more evidence for trust boost)
â†’ Log event for human review

SEVERITY 2 (Moderate anomaly):
â†’ Reduce trust score by 0.2
â†’ Display warning to users: "âš ï¸ Unusual voting pattern detected"
â†’ Require additional evidence for score to increase

SEVERITY 3 (Severe anomaly):
â†’ Freeze trust score (prevent further changes)
â†’ Quarantine rumor (remove from main feed)
â†’ Notify moderators for investigation
â†’ Mark all participating accounts for enhanced monitoring

SEVERITY 4 (Critical attack):
â†’ Rollback to last checkpoint
â†’ Suspend all involved accounts
â†’ Publish incident report (transparency)8.2 Reputation-Weighted VotingTHE CORE IDEA: Not all votes are equal. Weight votes by source reliability.Standard voting: Each vote counts as 1.0

Reputation-weighted voting:
vote_weight = base_weight Ã— reputation_multiplier Ã— recency_factor

Where:
- base_weight = 1.0 (baseline)
- reputation_multiplier = source_reliability_score (0.0-2.0)
  - New users: 0.5Ã— weight
  - Average users: 1.0Ã— weight  
  - Expert users (high historical accuracy): 1.5-2.0Ã— weight
  
- recency_factor = activity_recency
  - Active in last week: 1.0Ã—
  - Inactive 1 month: 0.7Ã—
  - Inactive 3 months: 0.4Ã—

Example:
- Expert user (S=0.9) active yesterday: weight = 1.0 Ã— 1.8 Ã— 1.0 = 1.8
- New user (S=0.3) joined today: weight = 1.0 Ã— 0.6 Ã— 1.0 = 0.6
- Dormant user (S=0.7) inactive 2 months: weight = 1.0 Ã— 1.4 Ã— 0.5 = 0.7WHY THIS DEFEATS SYBIL ATTACKS:Attack scenario:
- Attacker creates 100 new bot accounts
- Each bot has S=0.3 (new user, no history)
- Each vote weighted: 0.6

Total attack weight: 100 Ã— 0.6 = 60

Defense:
- 10 expert users (S=0.9) counter-vote
- Each vote weighted: 1.8

Total defense weight: 10 Ã— 1.8 = 18

Even though attackers have 10Ã— more votes, experts have 3Ã— weight per vote.
Attackers need 30+ votes to equal each expert vote.

Economic cost:
- 100 bot accounts Ã— 5 tokens per vote = 500 tokens
- 10 expert votes Ã— 5 tokens per vote = 50 tokens

Experts spend 10Ã— less to achieve 30% of attacker's weight.8.3 Evidence Graph AnalysisBUILDING THE EVIDENCE DEPENDENCY GRAPH:Create directed graph where:
- Nodes = Rumors + Evidence pieces
- Edges = "supports" or "contradicts" relationships

Example graph:
          [Photo of email]
                 â†“ supports
            [Rumor: Wifi down]
                 â†‘ supports         â†‘ contradicts
          [IT testimony]      [Student: "I have wifi"]

Graph metrics reveal manipulation:

1. ISOLATED SUBGRAPHS (Suspicious)
   - 10 rumors that only reference each other
   - No external corroboration
   - Likely coordinated fake narrative

2. CIRCULAR REFERENCES (Suspicious)
   - Rumor A supports B, B supports C, C supports A
   - No external grounding
   - Self-reinforcing fiction

3. SINGLE-POINT-OF-FAILURE (Suspicious)
   - 20 rumors all depend on single unverified claim
   - If root is false, entire tree collapses

Detection algorithm:
- Strongly connected components (find circular references)
- Eigenvector centrality (find critical dependencies)
- Betweenness centrality (find bottleneck evidence)9. USER INTERFACE & EXPERIENCE DESIGN9.1 The Trust Score Display PhilosophyPRINCIPLE: Transparency Without OverwhelmingUsers should understand:

What the score means (How trustworthy?)
Why it has that score (What evidence?)
How certain we are (Confidence level)
How fresh the information is (Temporal relevance)
But NOT be overwhelmed with technical details.THE TRUST INDICATOR HIERARCHY:LEVEL 1: At-a-Glance (Always Visible)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Trust: 87/100 â­â­â­â­             â”‚
â”‚ Confidence: HIGH âœ…                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LEVEL 2: Quick Summary (Tap to expand)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Evidence:                           â”‚
â”‚ ğŸ“§ 1 Official Document              â”‚
â”‚ ğŸ“¸ 2 Photos                         â”‚
â”‚ ğŸ‘¥ 5 Witnesses                      â”‚
â”‚                                     â”‚
â”‚ Consensus: 12 verify / 1 dispute   â”‚
â”‚ Posted: 3 hours ago                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LEVEL 3: Full Details (Tap "See breakdown")
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Trust Score Components:             â”‚
â”‚                                     â”‚
â”‚ Veracity:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 82%        â”‚
â”‚ Confidence:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 65%        â”‚
â”‚ Relevance:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 95%        â”‚
â”‚ Source:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 71%        â”‚
â”‚ Consensus:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 58%        â”‚
â”‚                                     â”‚
â”‚ [View Full Audit Trail]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜9.2 The Submission Flow: Making Evidence EasySTEP 1: Submit Rumor
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Share Campus News                      â”‚
â”‚                                        â”‚
â”‚ What's happening?                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ The library will be closed     â”‚   â”‚
â”‚ â”‚ tomorrow for emergency repairs â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚
â”‚ Category: [ğŸ“š Facilities]             â”‚
â”‚                                        â”‚
â”‚ How confident are you?                 â”‚
â”‚ â— Very sure (stake 20 tokens) â†       â”‚
â”‚ â—‹ Pretty sure (stake 10 tokens)       â”‚
â”‚ â—‹ Heard from someone (stake 5 tokens) â”‚
â”‚                                        â”‚
â”‚ [Next: Add Evidence]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜STEP 2: Add Evidence (Guided)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Make Your Claim Stronger               â”‚
â”‚                                        â”‚
â”‚ Adding evidence increases trust        â”‚
â”‚ and protects your staked tokens!       â”‚
â”‚                                        â”‚
â”‚ Evidence type:                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚ â”‚ ğŸ“¸ Photo â”‚ â”‚ ğŸ“§ Email â”‚            â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚ â”‚ ğŸ¥ Video â”‚ â”‚ ğŸ“„ Doc   â”‚            â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                        â”‚
â”‚ Or describe what you saw:              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ I saw maintenance crews         â”‚   â”‚
â”‚ â”‚ setting up barriers outside    â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚
â”‚ [Submit] [Add More Evidence]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜STEP 3: Stake Confirmation
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Review & Submit                        â”‚
â”‚                                        â”‚
â”‚ Your claim:                            â”‚
â”‚ "Library closed tomorrow for repairs"  â”‚
â”‚                                        â”‚
â”‚ Evidence: 1 photo, 1 description       â”‚
â”‚ Predicted trust score: ~70-80/100     â”‚
â”‚                                        â”‚
â”‚ You're staking: 20 tokens              â”‚
â”‚                                        â”‚
â”‚ If verified TRUE:    +10 tokens        â”‚
â”‚ If verified FALSE:   -20 tokens        â”‚
â”‚ If disputed:         +5 tokens         â”‚
â”‚                                        â”‚
â”‚ Your token balance: 85 â†’ 65 (staked)  â”‚
â”‚                                        â”‚
â”‚ [âœ“ Confirm] [â† Back]                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜9.3 The Verification Flow: Encouraging Quality InputVERIFICATION INTERFACE:â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“š Library closed tomorrow             â”‚
â”‚ Trust: 45/100 âš ï¸ (NEEDS VERIFICATION) â”‚
â”‚                                        â”‚
â”‚ Original claim:                        â”‚
â”‚ "Emergency repairs closing library"    â”‚
â”‚                                        â”‚
â”‚ Evidence so far:                       â”‚
â”‚ ğŸ“¸ 1 photo (unclear, far away)        â”‚
â”‚ ğŸ‘¤ Submitted by user with 65% accuracyâ”‚
â”‚                                        â”‚
â”‚ Can you verify this?                   â”‚
â”‚                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ âœ… I can VERIFY (add evidence) â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ âŒ I can DISPUTE (add evidence)â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ ğŸ¤· DON'T KNOW (skip)           â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚
â”‚ Stake 5 tokens to verify               â”‚
â”‚ Win 7-10 tokens if correct!            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜EVIDENCE SUBMISSION (SMART GUIDANCE):When user clicks "VERIFY":

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add Your Evidence                      â”‚
â”‚                                        â”‚
â”‚ ğŸ’¡ TIP: Strong evidence gets higher   â”‚
â”‚ rewards! Photos and documents are      â”‚
â”‚ better than just descriptions.         â”‚
â”‚                                        â”‚
â”‚ What can you provide?                  â”‚
â”‚                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ ğŸ“¸ I have a photo              â”‚   â”‚
â”‚ â”‚    Potential trust boost: +15% â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ ğŸ“§ I received an email         â”‚   â”‚
â”‚ â”‚    Potential trust boost: +25% â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ ğŸ‘ï¸ I witnessed it personally   â”‚   â”‚
â”‚ â”‚    Potential trust boost: +8%  â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚
â”‚ [Continue]                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜9.4 The Feed: Intelligent RankingFEED RANKING ALGORITHM:Feed_Score = trust_score Ã— relevance Ã— personalization - fatigue_penalty

Where:
- trust_score = the computed trust score (0-100)
- relevance = temporal_relevance Ã— topic_relevance
  - temporal: How soon is this relevant? (event timing)
  - topic: Does user care about this category?
  
- personalization = user_interest_score
  - Based on past engagement with similar rumors
  - Past accuracy in this topic area
  
- fatigue_penalty = duplicate_content_penalty
  - If similar rumors already shown: -20 points
  - Prevents feed spam

Sort by Feed_Score, descendingFEED DISPLAY:â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”¥ Trending Now                        â”‚
â”‚                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ğŸš¨ Campus WiFi Down Tomorrow     â”‚ â”‚
â”‚ â”‚ Trust: 87/100 â­â­â­â­          â”‚ â”‚
â”‚ â”‚ ğŸ“§ Official IT email + 3 photos  â”‚ â”‚
â”‚ â”‚ 2 hours ago â€¢ 15 verifications   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ğŸ‰ Free Pizza in Student Union   â”‚ â”‚
â”‚ â”‚ Trust: 72/100 â­â­â­            â”‚ â”‚
â”‚ â”‚ ğŸ“¸ 2 photos from event           â”‚ â”‚
â”‚ â”‚ 30 min ago â€¢ 8 verifications     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ âš ï¸ Professor Smith Quiz Canceled â”‚ â”‚
â”‚ â”‚ Trust: 45/100 âš ï¸ UNVERIFIED     â”‚ â”‚
â”‚ â”‚ ğŸ‘¤ Single testimony, no evidence â”‚ â”‚
â”‚ â”‚ 1 hour ago â€¢ 2 verifications     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚
â”‚ [Show More]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜9.5 Gamification: Making Truth-Seeking FunACHIEVEMENT SYSTEM:Badges earned through honest participation:

ğŸ” TRUTH SEEKER (Bronze)
Verify 10 rumors correctly
Reward: +50 tokens

ğŸ¯ SHARPSHOOTER (Silver)
Maintain >80% accuracy over 30 days
Reward: +100 tokens, 1.2Ã— vote weight

ğŸ† FACT MASTER (Gold)
First to verify 5 major rumors with evidence
Reward: +200 tokens, "Expert" badge visible on profile

âš¡ EARLY BIRD (Special)
Verify rumors within first hour, 20 times
Reward: Priority in feed rankings

ğŸ›¡ï¸ DEFENDER (Special)
Successfully dispute 5 false rumors
Reward: +150 tokens, enhanced anomaly detection accessLEADERBOARD:â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ† Campus Truth Leaders (This Week)   â”‚
â”‚                                        â”‚
â”‚ 1. ğŸ¥‡ Anonymous_Eagle                 â”‚
â”‚    Accuracy: 94% â€¢ 47 verifications   â”‚
â”‚    Tokens earned: +340                 â”‚
â”‚                                        â”‚
â”‚ 2. ğŸ¥ˆ Anonymous_Hawk                  â”‚
â”‚    Accuracy: 91% â€¢ 38 verifications   â”‚
â”‚    Tokens earned: +285                 â”‚
â”‚                                        â”‚
â”‚ 3. ğŸ¥‰ Anonymous_Falcon                â”‚
â”‚    Accuracy: 89% â€¢ 52 verifications   â”‚
â”‚    Tokens earned: +270                 â”‚
â”‚                                        â”‚
â”‚ ...                                    â”‚
â”‚                                        â”‚
â”‚ 42. You                                â”‚
â”‚     Accuracy: 78% â€¢ 12 verifications  â”‚
â”‚     Tokens earned: +85                 â”‚
â”‚     â†‘ +5 places from yesterday!        â”‚
â”‚                                        â”‚
â”‚ [View Full Leaderboard]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜10. TECHNICAL IMPLEMENTATION STACK10.1 Recommended TechnologiesFRONTEND:
Framework: React Native
- Cross-platform (iOS, Android, Web)
- Performance optimized for mobile
- Rich ecosystem for UI components

State Management: Redux + Redux-Saga
- Centralized state management
- Side-effect handling for async operations

Cryptography Library: noble-curves + noble-hashes
- Zero-knowledge proof generation (zk-SNARKs)
- Cryptographic primitives for identity
- Browser-compatible, no native dependenciesBACKEND:
API Layer: Node.js + Express
- REST API for client-server communication
- WebSocket for real-time updates

Database: PostgreSQL + Redis
- PostgreSQL: Relational data (users, rumors, votes)
- Redis: Caching layer, real-time scoring

Blockchain Layer: Ethereum (or Polygon for lower fees)
- Smart contracts for token economy
- Immutable ledger for checkpoints
- Trustless verification

Alternative (if blockchain too complex): 
- Git-backed immutable ledger
- Merkle trees for tamper-proof historyDECENTRALIZED STORAGE:
Evidence Storage: IPFS (InterPlanetary File System)
- Content-addressed storage (files stored by hash)
- Distributed (no central server)
- Censorship-resistant

Metadata: OrbitDB (decentralized database on IPFS)
- Distributed key-value store
- Eventually consistent
- Conflict-free replicated data types (CRDTs)SECURITY & PRIVACY:
Zero-Knowledge Proofs: SnarkJS
- zk-SNARK circuit compilation
- Proof generation and verification
- Privacy-preserving credentials

End-to-End Encryption: libsodium
- Encrypted communication channels
- User data protectionMACHINE LEARNING (Bot Detection):
Anomaly Detection: Python + scikit-learn
- Behavioral clustering (DBSCAN, K-means)
- Time-series analysis (ARIMA for temporal patterns)
- Graph analysis (NetworkX for Sybil detection)

Real-time Inference: TensorFlow.js
- Client-side behavior monitoring
- Lightweight models for mobile devices10.2 Detailed System Architecture Diagramâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLIENT LAYER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Mobile    â”‚  â”‚     Web     â”‚  â”‚   Desktop   â”‚        â”‚
â”‚  â”‚     App     â”‚  â”‚   Browser   â”‚  â”‚     App     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                 â”‚                 â”‚                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                           â”‚                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Load Balancer  â”‚
                   â”‚   (NGINX)       â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API GATEWAY                                â”‚
â”‚                 (Authentication, Rate Limiting)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                  â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ Trust    â”‚     â”‚ Identity  â”‚     â”‚ Evidence   â”‚
    â”‚ Score    â”‚     â”‚ Manager   â”‚     â”‚ Verifier   â”‚
    â”‚ Engine   â”‚     â”‚ (zk-SNARK)â”‚     â”‚            â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ Anomaly  â”‚     â”‚ Temporal  â”‚     â”‚ Game       â”‚
    â”‚ Detector â”‚     â”‚ Decay Mgr â”‚     â”‚ Theory Eng â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA LAYER                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ PostgreSQL   â”‚   â”‚    Redis     â”‚   â”‚  Blockchain  â”‚   â”‚
â”‚   â”‚ (Relational) â”‚   â”‚   (Cache)    â”‚   â”‚  (Immutable) â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚     IPFS     â”‚   â”‚   OrbitDB    â”‚                       â”‚
â”‚   â”‚  (Evidence)  â”‚   â”‚  (Metadata)  â”‚                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ANALYTICS & MONITORING                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Prometheus  â”‚   â”‚   Grafana    â”‚   â”‚  ELK Stack   â”‚   â”‚
â”‚   â”‚  (Metrics)   â”‚   â”‚(Dashboards)  â”‚   â”‚   (Logs)     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜10.3 Database Schema (PostgreSQL)sql-- USERS (Anonymous credential holders)
CREATE TABLE users (
  user_id UUID PRIMARY KEY,
  credential_hash VARCHAR(64) UNIQUE NOT NULL,  -- zk-SNARK credential
  created_at TIMESTAMP NOT NULL,
  last_active TIMESTAMP,
  
  -- Reputation tracking
  total_submissions INT DEFAULT 0,
  verified_accurate INT DEFAULT 0,
  verified_false INT DEFAULT 0,
  reputation_score DECIMAL(3,2) DEFAULT 0.50,
  token_balance INT DEFAULT 100,
  
  -- Anti-Sybil
  behavioral_fingerprint JSONB,
  bot_score DECIMAL(3,2) DEFAULT 0.0,
  account_status VARCHAR(20) DEFAULT 'active'  -- active, flagged, suspended
);

-- RUMORS
CREATE TABLE rumors (
  rumor_id UUID PRIMARY KEY,
  submitter_id UUID REFERENCES users(user_id),
  content TEXT NOT NULL,
  category VARCHAR(50),
  
  -- Trust scoring
  veracity_score DECIMAL(3,2),
  confidence_score DECIMAL(3,2),
  temporal_relevance DECIMAL(3,2),
  source_reliability DECIMAL(3,2),
  network_consensus DECIMAL(3,2),
  final_trust_score DECIMAL(5,2),
  
  -- Metadata
  created_at TIMESTAMP NOT NULL,
  event_time TIMESTAMP,  -- When the rumored event occurs/occurred
  status VARCHAR(20) DEFAULT 'active',  -- active, deprecated, archived
  stake_amount INT NOT NULL,
  
  -- Immutability
  content_hash VARCHAR(64) NOT NULL,  -- SHA-256 of content
  merkle_root VARCHAR(64),  -- For checkpoint verification
  
  INDEX idx_trust_score (final_trust_score DESC),
  INDEX idx_created_at (created_at DESC),
  INDEX idx_category (category)
);

-- EVIDENCE
CREATE TABLE evidence (
  evidence_id UUID PRIMARY KEY,
  rumor_id UUID REFERENCES rumors(rumor_id),
  submitter_id UUID REFERENCES users(user_id),
  
  evidence_type VARCHAR(20),  -- photo, video, document, testimony
  evidence_hash VARCHAR(64) NOT NULL,  -- IPFS hash or content hash
  ipfs_cid VARCHAR(100),  -- IPFS content identifier
  
  -- Quality metrics
  evidence_weight DECIMAL(3,2),
  quality_score DECIMAL(3,2),  -- Automated verification score
  
  created_at TIMESTAMP NOT NULL,
  
  UNIQUE(evidence_hash)  -- Prevent duplicate evidence
);

-- VERIFICATIONS (Votes)
CREATE TABLE verifications (
  verification_id UUID PRIMARY KEY,
  rumor_id UUID REFERENCES rumors(rumor_id),
  verifier_id UUID REFERENCES users(user_id),
  
  vote_type VARCHAR(10),  -- support, dispute
  stake_amount INT NOT NULL,
  evidence_provided UUID REFERENCES evidence(evidence_id),
  
  -- Vote weight (reputation-adjusted)
  vote_weight DECIMAL(3,2),
  
  created_at TIMESTAMP NOT NULL,
  
  -- Nullifier for preventing double-voting
  nullifier_hash VARCHAR(64) UNIQUE NOT NULL,
  
  UNIQUE(rumor_id, verifier_id)  -- One vote per user per rumor
);

-- RUMOR DEPENDENCIES (Graph structure)
CREATE TABLE rumor_references (
  reference_id UUID PRIMARY KEY,
  source_rumor_id UUID REFERENCES rumors(rumor_id),
  target_rumor_id UUID REFERENCES rumors(rumor_id),
  reference_type VARCHAR(20),  -- supports, contradicts, related
  
  created_at TIMESTAMP NOT NULL,
  
  UNIQUE(source_rumor_id, target_rumor_id)
);

-- CHECKPOINTS (Immutable snapshots)
CREATE TABLE checkpoints (
  checkpoint_id UUID PRIMARY KEY,
  checkpoint_time TIMESTAMP NOT NULL,
  merkle_root VARCHAR(64) NOT NULL,  -- Root of Merkle tree of all rumors
  
  -- Snapshot data (compressed JSON)
  snapshot_data JSONB,
  
  -- Blockchain anchoring (optional)
  blockchain_tx_hash VARCHAR(66),
  
  UNIQUE(checkpoint_time)
);

-- ANOMALY LOGS
CREATE TABLE anomaly_logs (
  log_id UUID PRIMARY KEY,
  detected_at TIMESTAMP NOT NULL,
  anomaly_type VARCHAR(50),  -- vote_clustering, bot_behavior, etc.
  severity INT,  -- 1-4
  
  -- Affected entities
  rumor_ids UUID[],
  user_ids UUID[],
  
  -- Detection details
  detection_metadata JSONB,
  
  -- Response taken
  action_taken VARCHAR(100),
  
  INDEX idx_detected_at (detected_at DESC)
);10.4 Smart Contract (Ethereum/Solidity)solidity// SPDX-License-Identifier: MIT
pragma solidity ^0.8.0;

contract CampusRumorToken {
    
    // Token tracking
    mapping(address => uint256) public balances;
    mapping(address => mapping(bytes32 => uint256)) public stakes;  // user => rumor => amount
    
    // Rumor registry
    struct Rumor {
        bytes32 contentHash;
        address submitter;
        uint256 stakeAmount;
        uint256 timestamp;
        bool resolved;
        bool outcomeVerified;  // true if verified, false if refuted
    }
    
    mapping(bytes32 => Rumor) public rumors;
    
    // Events
    event RumorSubmitted(bytes32 indexed rumorId, address indexed submitter, uint256 stake);
    event RumorVerified(bytes32 indexed rumorId, address indexed verifier, uint256 stake, bool support);
    event RumorResolved(bytes32 indexed rumorId, bool verified, uint256 totalPayout);
    event TokensAwarded(address indexed user, uint256 amount);
    event TokensSlashed(address indexed user, uint256 amount);
    
    // Initial token allocation
    constructor() {
        // Each new user gets 100 tokens when credential is issued
    }
    
    function submitRumor(
        bytes32 _rumorId,
        bytes32 _contentHash,
        uint256 _stakeAmount
    ) external {
        require(balances[msg.sender] >= _stakeAmount, "Insufficient balance");
        require(_stakeAmount >= 5 && _stakeAmount <= 50, "Invalid stake amount");
        require(rumors[_rumorId].submitter == address(0), "Rumor already exists");
        
        // Lock stake
        balances[msg.sender] -= _stakeAmount;
        stakes[msg.sender][_rumorId] = _stakeAmount;
        
        // Record rumor
        rumors[_rumorId] = Rumor({
            contentHash: _contentHash,
            submitter: msg.sender,
            stakeAmount: _stakeAmount,
            timestamp: block.timestamp,
            resolved: false,
            outcomeVerified: false
        });
        
        emit RumorSubmitted(_rumorId, msg.sender, _stakeAmount);
    }
    
    function verifyRumor(
        bytes32 _rumorId,
        bool _support,
        uint256 _stakeAmount,
        bytes memory _zkProof  // Zero-knowledge proof of credential
    ) external {
        require(rumors[_rumorId].submitter != address(0), "Rumor does not exist");
        require(!rumors[_rumorId].resolved, "Rumor already resolved");
        require(balances[msg.sender] >= _stakeAmount, "Insufficient balance");
        require(_stakeAmount >= 2 && _stakeAmount <= 20, "Invalid stake amount");
        
        // Verify zk-proof (user has valid credential and hasn't voted before)
        require(_verifyZKProof(_zkProof, _rumorId), "Invalid proof");
        
        // Lock stake
        balances[msg.sender] -= _stakeAmount;
        stakes[msg.sender][_rumorId] = _stakeAmount;
        
        emit RumorVerified(_rumorId, msg.sender, _stakeAmount, _support);
    }
    
    function resolveRumor(
        bytes32 _rumorId,
        bool _verified,
        address[] memory _winners,
        address[] memory _losers
    ) external {
        // Called by trusted oracle/backend after trust score finalized
        require(rumors[_rumorId].submitter != address(0), "Rumor does not exist");
        require(!rumors[_rumorId].resolved, "Already resolved");
        
        Rumor storage rumor = rumors[_rumorId];
        rumor.resolved = true;
        rumor.outcomeVerified = _verified;
        
        // Calculate total losing stakes
        uint256 loserPool = 0;
        for (uint i = 0; i < _losers.length; i++) {
            uint256 stake = stakes[_losers[i]][_rumorId];
            loserPool += stake;
            stakes[_losers[i]][_rumorId] = 0;  // Slash stake
            emit TokensSlashed(_losers[i], stake);
        }
        
        // Distribute rewards to winners
        uint256 bonusPerWinner = loserPool / _winners.length;
        for (uint i = 0; i < _winners.length; i++) {
            uint256 originalStake = stakes[_winners[i]][_rumorId];
            uint256 payout = originalStake + (originalStake / 2) + bonusPerWinner;
            
            balances[_winners[i]] += payout;
            stakes[_winners[i]][_rumorId] = 0;
            emit TokensAwarded(_winners[i], payout);
        }
        
        emit RumorResolved(_rumorId, _verified, loserPool);
    }
    
    function _verifyZKProof(bytes memory _proof, bytes32 _rumorId) internal view returns (bool) {
        // Verify zk-SNARK proof
        // Proof must show:
        // 1. User possesses valid campus credential
        // 2. Nullifier for (user, rumorId) hasn't been used
        // 3. User's identity remains hidden
        
        // Simplified for example - real implementation uses Groth16 or PLONK verification
        return true;
    }
    
    // Token minting for rewards (controlled by governance)
    function mint(address _to, uint256 _amount) external {
        // Only callable by trusted reward system
        balances[_to] += _amount;
    }
}11. MATHEMATICAL PROOF OF GAME-THEORETIC SECURITY11.1 Formal ModelDEFINITIONS:Let:
- N = total number of users in the system
- L = number of coordinated liars (L < N)
- H = number of honest users (H = N - L)
- E_h = average evidence quality of honest users [0, 1]
- E_l = average evidence quality of liars [0, 1]
- T_l = total token pool of liars
- T_h = total token pool of honest users
- w_e = evidence weight function: quality â†’ score contribution
- C_attack = cost to maintain false narrative for time t
- R_attack = reward if attack succeedsASSUMPTION SET:A1: Honest majority
    H â‰¥ 2L (at least 2/3 honest users)

A2: Evidence quality asymmetry
    E_h > E_l (truth has better evidence than lies)
    Specifically: E_h â‰¥ 0.6, E_l â‰¤ 0.4

A3: Token economics
    T_h = H Ã— avg_tokens â‰¥ T_l = L Ã— avg_tokens
    (Honest users collectively wealthier due to A1)

A4: Rational actors
    All users maximize expected utility: E[Reward] - Cost

A5: Evidence weight function is strictly increasing
    w_e(E_h) > w_e(E_l)11.2 Theorem: Nash Equilibrium is Truth-TellingTHEOREM:
Under assumptions A1-A5, the unique Nash Equilibrium strategy for rational users is truth-telling with evidence provision.PROOF:Step 1: Define the gamePlayers: N users
Strategy space for each user i:
  S_i = {(submit_truth, evidence), (submit_lie, evidence), (abstain)}
  
Payoff function for user i playing strategy s_i given others play s_-i:
  U_i(s_i, s_-i) = E[Token_reward] - Token_cost - Reputation_costStep 2: Analyze payoffs for truth-tellingTruth-telling strategy: (submit_truth, evidence_h)

Expected payoff:
U_truth = P(verified) Ã— [stake + bonus] - stake

Where:
- P(verified) = probability rumor is verified as true
- stake = tokens staked
- bonus = rewards from correct verification

Computing P(verified):
Given veracity score formula:
  V = Î£(evidence_weight Ã— quality Ã— recency) / (1 + contradiction)

For truth with honest evidence:
  V_truth = w_e(E_h) Ã— n_h / (1 + w_e(E_l) Ã— n_l)

Where n_h = number of honest verifiers, n_l = number of lying verifiers

Under A1 (H â‰¥ 2L):
  Expected n_h / n_l â‰¥ 2

Under A2 (E_h > E_l):
  w_e(E_h) / w_e(E_l) â‰¥ 1.5 (from evidence weighting)

Therefore:
  V_truth â‰¥ (1.5 Ã— 2) / (1 + 1) = 3/2 = 1.5 > 0.7 (verification threshold)

So: P(verified | truth) â‰ˆ 0.95 (high probability)

Expected payoff:
  U_truth = 0.95 Ã— (stake Ã— 1.5) - stake
          = 0.95 Ã— 1.5 Ã— stake - stake
          = 1.425 Ã— stake - stake
          = 0.425 Ã— stake
  
Positive expected return: +42.5% on stakeStep 3: Analyze payoffs for lyingLying strategy: (submit_lie, evidence_l)

Expected payoff:
U_lie = P(verified_false) Ã— [-stake] + P(verified_true) Ã— [stake + bonus]

Computing P(verified_false | lie):
Given honest users will dispute with better evidence:

V_lie = w_e(E_l) Ã— n_l / (1 + w_e(E_h) Ã— n_h)

Under A1 and A2:
  V_lie â‰¤ 1.0 Ã— L / (1 + 1.5 Ã— 2H)
       â‰¤ 1.0 Ã— L / (1 + 3H)

Since H â‰¥ 2L (A1):
  V_lie â‰¤ L / (1 + 6L) = 1/(1 + 6) = 1/7 â‰ˆ 0.14 < 0.3 (false threshold)

So: P(verified_false | lie) â‰ˆ 0.90 (high probability of being caught)

Expected payoff:
  U_lie = 0.90 Ã— (-stake) + 0.10 Ã— (stake Ã— 1.5)
        = -0.90 Ã— stake + 0.15 Ã— stake
        = -0.75 Ã— stake

Negative expected return: -75% on stakeStep 4: Compare strategiesExpected utility comparison:
  U_truth = +0.425 Ã— stake
  U_lie = -0.75 Ã— stake
  
Difference:
  U_truth - U_lie = 0.425 Ã— stake + 0.75 Ã— stake = 1.175 Ã— stake > 0

Truth-telling dominates lying by significant margin (+117.5% relative difference)

For any rational actor: s* = truth-telling

This holds for all users, thus truth-telling is a Nash Equilibrium.Step 5: Prove uniquenessCould lying be an equilibrium if all users lie?

If all users coordinate to lie:
- No contradicting evidence initially
- V_lie appears high initially

But:
1. First honest user to defect (tell truth) gains:
   - Evidence weight advantage: w_e(E_h) > w_e(E_l)
   - All staked tokens from liars when rumor is refuted
   - Defection payoff >> coordination payoff

2. Lying equilibrium is unstable (dominant strategy to defect)

3. Only stable equilibrium is universal truth-telling

Q.E.D.11.3 Robustness AnalysisCLAIM: System resists coordinated attacks up to L = H/2 attackersPROOF BY CONTRADICTION:Assume coordinated attack succeeds with L = H/2 liars.

Attack requirements for success:
1. Maintain V_lie > 0.7 (verification threshold)
2. Sustain narrative for t â‰¥ 48 hours (resolution time)
3. Profit: R_attack > C_attack

Analyzing requirement 1:
V_lie = w_e(E_l) Ã— L / (1 + w_e(E_h) Ã— H)
     = 1.0 Ã— (H/2) / (1 + 1.5 Ã— H)  [substituting L = H/2]
     = H/2 / (1 + 1.5H)
     = H / (2 + 3H)

For large H (realistic campus size: H > 50):
  V_lie â‰ˆ H / 3H = 1/3 â‰ˆ 0.33 < 0.7

Requirement 1 FAILS: Cannot achieve verification threshold.

Analyzing requirement 2 (even if requirement 1 somehow succeeds):
Cost to sustain for 48 hours:

C_attack = Initial_stakes + Maintenance_stakes + Opportunity_cost

Initial stakes:
  L users Ã— average_stake = (H/2) Ã— 10 tokens = 5H tokens

Maintenance stakes (honest users continuously dispute):
  Each hour: H users stake 5 tokens average
  48 hours: 48 Ã— H Ã— 5 = 240H tokens needed to counter

Total C_attack = 5H + 240H = 245H tokens

But total liar token pool: T_l = L Ã— 100 = 50H tokens

Requirement 2 FAILS: Insufficient resources to sustain attack.

CONTRADICTION: Cannot satisfy attack requirements with L = H/2.

Therefore: System is robust against coordinated attacks up to L < H/2.

Q.E.D.


KEY METRICS TO TRACK:Engagement Metrics:
- Daily active users (DAU)
- Rumors submitted per day
- Verification rate (% of rumors that get verified)
- Evidence attachment rate

Quality Metrics:
- Trust score accuracy (compare to ground truth when available)
- False positive rate (true rumors marked false)
- False negative rate (false rumors marked true)
- Time to resolution (how long until consensus reached)

Security Metrics:
- Bot detection rate (accounts flagged / total accounts)
- Successful attack attempts (0 is the goal!)
- Sybil attack attempts detected
- Anomaly alerts per day

Economic Metrics:
- Token velocity (how often tokens change hands)
- Average stake amounts
- Token inequality (Gini coefficient - should stay moderate)
- Reward distribution fairness
